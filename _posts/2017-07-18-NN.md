---
layout: post_layout
title: 神经网络
time: 2017年07月04日 星期二
location: 北京
pulished: true
tag: ML
excerpt_separator: "----"
---

　　最近在使用github上的一个开源库[DeepQA](https://github.com/Conchylicultor/DeepQA)来对聊天语料进行训练，以使得机器人可以获得跟人对话的能力，从而摆脱对图灵的依赖，这个库用到了神经网络方面的知识，它是用了两个LSTM网络来组成encoder和decoder，来训练模型，主要思想应该是利用encoder来将问句转化为一个向量，然后再将该向量作为输出，输入到decoder，decoder输出即为回答，但具体怎么做还是要参考论文[Sequence to Sequence Learning with Neural Networks](http://cn.arxiv.org/pdf/1409.3215.pdf)，当然作为一个神经网络初学者，到这里，你可能想问LSTM又是什么，encoder和decoder又是干什么的，我当初也是这么迷茫，不要着急，咱们一步步地学，这一节我们就来学习最简单的神经网络模型。

----

# **神经网络** #

　　我们都知道人的大脑里有很多神经元，一个连着一个传递着大量的信息，而神经网络就是想要模拟人脑中的神经网络，来达到类似人脑一样学习的目的，那么，我们先来看最简单的一个神经网络长什么样。

![]({{site.pictureurl}}78.jpg?raw=true)

　　神经网络通常有三层，输入层、隐藏层（隐藏层也可以有多层，这里最简单的情况只有一层隐藏层）、输出层，每一层上都有一些圆圈，这些圆圈就是我们的“神经元”，那么它的结构是怎样的呢？如下图：

![]({{site.pictureurl}}79.jpg?raw=true)

　　每一个神经元，都会接受一些输入，然后经过一些计算后，再输出一个数据，那么我们需要搞清楚，这些输入数据怎么来，是怎么计算从而得出的输出。

　　首先是输入，对于输入层的神经元，他们没有接受输入的过程，每个神经元节点自己就是一个输入数据。而从隐藏层开始直到输出层，每一个神经元接受的输入实际上就是前面一层的各个神经元节点的输出，例如输入层有三个节点，那么对隐藏层的神经元来说，每个神经元接受的输入就是输入层的三个节点的值，例如x1、x2、x3，隐藏层假设有4个节点，这4个节点的输出便是输出层每个节点的输入。

　　知道了输入数据是什么，我们接下来要弄清楚怎么从输入得到输出，为了方便描述，我还是自己画图并计算吧。我们来看下图：

![]({{site.pictureurl}}80.jpg?raw=true)

　　首先上一层的每一个神经元都会有一条线连接到下一层的每一个神经元，这条线上都有一个独特的权值（这也是需要我们最后学习获得的参数），那么一个神经元的输出的计算方式就是上一层的所有输入的一个加权求和，但是在图中我们可以看到多了一个输入是b，它其实也是一个需要学习获得的参数（可以当做一个特殊的神经元，不会接受上一层的输入来计算获得输出值，他输出的值就是它本身，是需要学习获得的），加权求和之后我们得到了y，然后再对y计算一个sigmoid函数，就得到了该神经元节点的输出了，看到这里，你是不是觉得有点似曾相识的感觉，这一个神经元的计算过程不就是逻辑回归的过程吗，一模一样啊！我们知道逻辑回归主要用来处理分类问题，而神经网络相当于将很多个逻辑回归的过程组合起来，组成了一个相当复杂的黑箱，你只要告诉它输入，输出，他就能学习到输入与输出之间的函数关系，也就是通过学习，这个黑箱可以逼近任意的函数，理论上，只要数据足够多，神经元数量足够大，神经网络就能学到你需要的东西

### **激活函数** ###

　　我们看到，在加权求和之后，我们进行了sigmoid计算，才得到最终的输出，这在逻辑回归中也是这样做的，这个sigmoid函数就是我们所说的激活函数，那么为什么一定要多这一步激活函数呢？激活函数的作用是什么呢？这个问题，可以参考如下知乎的回答，还是解释的蛮清楚的。[https://www.zhihu.com/question/22334626/answer/21036590](https://www.zhihu.com/question/22334626/answer/21036590)

　　总结下来就是：激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。我们都知道，有的时候一条直线并不能将数据分类，但是引入非线性的因素就变得可能了。更何况，神经网络中的每一个神经元都是一个非线性的分类过程，将多个非线性的分类组合起来，就是它能逼近任意函数的原因了。

　　当然激活函数不只有sigmoid函数，还有tanh、softmax、Relu等等，见[http://www.cnblogs.com/steven-yang/p/6357775.html](http://www.cnblogs.com/steven-yang/p/6357775.html)

### **前向传播** ###

　　接下来我们来看看完整的神经网络计算过程，一个3\*3\*1的神经网络如下图：

![]({{site.pictureurl}}81.jpg?raw=true)

　　我们知道输入层的输入为x1，x2，x3，以及参数w和b（各边上的权值w以及常数节点的输出值b），我们要计算最后这个网络输出的y是多少，计算过程如下：

![]({{site.pictureurl}}82.jpg?raw=true)

　　这样给定输入，和各参数的值，我们就能得到输出，这个计算过程叫前向传播

### **反向传播** ###

　　然而，通常，在我们构建完神经网络后，参数w、b我们都是不知道的，这正是我们要学习获得的参数，而我们有的东西，只有两部分，一是构建好的神经网络（至于如何构建，也就是隐藏层有多少、每层选多少个神经元节点，这些都不是固定的，只能靠实践去探索，通常在特定场景下，输入层和输出层的节点个数是确定的），另一个是我们的训练数据，也就是一堆的(x,y)的样本（x和y都是向量，向量的维度对应了输入层和输出层的节点个数），而我们需要通过这两样已知的东西去求解w和b，这就是神经网络的学习过程。

　　神经网络的学习方法有很多，我们这里介绍最经典的一种，反向传播BP算法（Back Propagation）。

　　因为涉及到公式，而markdown写公式实在蛋疼，所以我借助[ufldl](http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)的讲解来说明吧，实在需要推导的地方，我就手写推导下。

#### **损失函数** ####

　　首先，我们是利用批量梯度下降的思路去求解各个参数，这跟逻辑回归参数求解的思路一样，那么首先我们需要定义损失函数，损失函数的定义如下：

![]({{site.pictureurl}}83.jpg?raw=true)

#### **参数初始化** ####
　　
![]({{site.pictureurl}}84.jpg?raw=true)

#### **参数更新** ####

![]({{site.pictureurl}}85.jpg?raw=true)

　　那么关键是如何求导，下面做详细讲解：

![]({{site.pictureurl}}86.jpg?raw=true)

　　看到这里，你肯定跟我开始看一样一脸懵逼，不是要求对w参数和b参数的求导么，这个残差是什么鬼，先不要着急，我们先按他说的理解，这个残差就是代表了这个节点对最终输出值的影响，它其实拿损失函数对这个节点的直接输出（即还没有使用激活函数，只是做了加权求和的输出）进行求导的结果。

　　上面这个求导过程，首先我们要搞清楚，损失函数是考虑了所有样本的，而这里的计算，我们是先取一个样本计算（因为全部样本的单个结果算出来，整体的也就能算出来了）。

　　首先，是拿这个样本的输入即x，以及现在的各个参数w、b，从前向后计算出所有神经元节点的输出值。然后计算每一层（不包括输入层）各个节点的残差。而输出层又和隐藏层的残差计算有些微不同，所以先看输出层的残差如何计算,上面图中直接给出了公司，我们来推导一下：

![]({{site.pictureurl}}87.jpg?raw=true)

　　推导过程还是比较简单，我们只要记住，最后的结果是，输出层的激活值与实际值的差乘以该节点未激活值的激活函数倒数的相反数

　　那么对于隐藏层的残差，又是如何计算呢？看下图（附带推导过程）：

![]({{site.pictureurl}}88.jpg?raw=true)

　　需要注意的是，这里推导的时候，是用nl-1层举例，进而得到l层与l+1层之间的关系

　　这里我们得到了所有节点的残差值，但是并没有得到最初我们想得到的损失函数对w和b参数的导数，下图给出了最终的计算公式：

![]({{site.pictureurl}}89.jpg?raw=true)

　　他的推导过程如下：

![]({{site.pictureurl}}90.jpg?raw=true)

　　注意，我的i和j与上图中颠倒了，因为我认为这里的i是前一层的节点序号，而上图认为i是下一层的节点序号，这不影响。

　　当然，上面不论是计算残差，还是计算w和b的导数，都是一个一个节点计算，或者一个一个参数计算，其实，我们可以将它转化为矩阵、向量的计算，将上面的计算过程用矩阵-向量法重写，如下：（注意这里还是针对一个样本的过程）

![]({{site.pictureurl}}91.jpg?raw=true)

　　有了一个样本的计算过程，那么我们的整个批量梯度下降学习过程就如下了：

![]({{site.pictureurl}}92.jpg?raw=true)

